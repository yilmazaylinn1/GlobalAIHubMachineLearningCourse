{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) How would you define Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the study of computer algorithms that improve automatically through experience and by the use of data.It is seen as a part of artificial intelligence. Machine learning is a category of algorithms that makes software programs more accurate at predicting results without explicit programming. The main purpose of machine learning is to create algorithms that can take input data and use statistical analysis to predict an output while updating output as new data emerges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning : A supervised learning algorithm learns from labeled training data, helps you to predict outcomes for unforeseen data. Successfully building, scaling, and deploying accurate supervised machine learning Data science model takes time and technical expertise from a team of highly skilled data scientists. Supervised learning allows you to collect data or produce a data output from the previous experience. Helps you to optimize performance criteria using experience\n",
    "\n",
    "Example : Linear Regression, Logistics Regression, Classification and K - Nearest Neighbour Algorithm, Decision Tree\n",
    "\n",
    "Unsupervised Learning : Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Instead, you need to allow the model to work on its own to discover information. It mainly deals with the unlabelled data. Unsupervised machine learning finds all kind of unknown patterns in data. Also Unsupervised methods help you to find features which can be useful for categorization.\n",
    "\n",
    "Example: Clustering, Apriori Algorithm and K- Means Clustering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What are the test and validation set, and why would you want to use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Set: It is a sub-data set used to evaluate the performance of the model obtained during the training phase. In addition, this dataset provides a test platform to determine which model is good and to adjust the optimal parameters for the models. The validation section is selected from the train data set.The algorithm is determined by choosing the right model on the train data set. In the validation section, the applied model is tried to be improved. Since very large data sets cannot be worked on continuously on Train data, a small part is taken and defined as validation.\n",
    "\n",
    "Test Set : It is used to evaluate the future performance of the model. The test set is the data set that the model has not encountered before, used to provide an unbiased evaluation of the final model that makes certain inferences from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is essential and is the most important step in machine learning algorithm implementations because a dataset model that is cleared of incomplete and unnecessary information better results for our model and allows us to make better predictions for the future.\n",
    "\n",
    "After the problem definition, we need to obtain data which will be appropriate for our case. The quality and quantity of data that you gather will directly determine how good our predictive model can be.\n",
    "\n",
    "Prepearing the Data : Data preparation, where we load our data into a suitable place and prepare it for use in our machine learning training. This is also a good time to do any pertinent visualizations of your data, to help you see if there are any relevant relationships between different variables you can take advantage of, as well as show you if there are any data imbalances.\n",
    "\n",
    "Exploratory Data Analysis (EDA) : Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n",
    "\n",
    "Pre - Processing :  is a very important step to prepare the data more appropriate for our model.\n",
    "\n",
    "* Duplicate Values : In most cases, the duplicates are removed so as to not give that particular data object an advantage or bias, when running machine learning algorithms.\n",
    "\n",
    "* Imbalanced Data : An Imbalanced dataset is one where the number of instances of a class(es) are significantly higher than another class(es), thus leading to an imbalance and creating rarer class(es).\n",
    "\n",
    "* Missing Value : It is the process of identifying missing data in the data set and filling them with median or average values appropriate for their location.\n",
    "\n",
    "* Outlier Detection : Outliers are data points that do not belong to a certain population. It is an abnormal observation that lies far away from other values. Briefly, an outlier is an observation that diverges from otherwise well-structured data. There are four common methods to deal with it: Standart Deviation, Box Plots / IQR Calculation, Isolation Forest\n",
    "\n",
    "* Feature Scaling : It allows us to reduce the variables to the ranges we have determined before inserting them into machine learning algorithms. There are two types of scaling the features: Standardization, Normalization\n",
    "\n",
    "* Bucketing (Binning) : Data binning, bucketing is a data pre-processing method used to minimize the effects of small observation errors (noisy data). The original data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin.\n",
    "\n",
    "* Feature Encoding : Feature encoding is basically performing transformations on the data such that it can be easily accepted as input for machine learning algorithms while still retaining its original meaning. There are two types of feature encoding: Nominal, Ordinal.\n",
    "\n",
    "* Train / Validation / Test Split : But before we start deciding the algorithm which should be used, it is always advised to split the dataset into 2 or sometimes 3 parts. Machine Learning algorithms, or any algorithm for that matter, has to be first trained on the data distribution available and then validated and tested, before it can be deployed to deal with real-world data.\n",
    "\n",
    "* Cross Validation : esting a prediction function on the same data is a critical mistake. It is called overfitting. To avoid it, the most effective way is to hold out part of the available data as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5 ) How you can explore and analyse countionus and discrete variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Variable : A discrete variable is a type of statistical variable that can assume only fixed number of distinct values and lacks an inherent order.\n",
    "\n",
    "Also known as a categorical variable, because it has separate, invisible categories. However no values can exist in-between two categories, i.e. it does not attain all the values within the limits of the variable. So, the number of permitted values that it can suppose is either finite or countably infinite. Hence if you are able to count the set of items, then the variable is said to be discrete.\n",
    "\n",
    "Continuous Variable: Continuous variable, as the name suggest is a random variable that assumes all the possible values in a continuum. Simply put, it can take any value within the given range. So, if a variable can take an infinite and uncountable set of values, then the variable is referred as a continuous variable.\n",
    "\n",
    "A continuous variable is one that is defined over an interval of values, meaning that it can suppose any values in between the minimum and maximum value. It can be understood as the function for the interval and for each function, the range for the variable may vary.\n",
    "\n",
    "We can analyze two variable using statistics methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is histogram and it shows the frequency of the data. A continues variable is one that is defined over an interval values so as we can see in the plot, we can say that plot variable type is continues variable type. Also the plot have bimodal distribution. In the preprocess we can detect the outliers and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
